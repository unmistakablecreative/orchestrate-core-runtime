{
    "filename": "/Users/srinivas/Orchestrate Github/Orchestrate_Hackathon/tools/browser_tool.py",
    "imports": [
        "import requests",
        "from bs4 import BeautifulSoup",
        "import re",
        "import json"
    ],
    "functions": {
        "search_web": {
            "params": [
                "query",
                "source"
            ],
            "body": "if source == \"duckduckgo\":\n    url = f\"https://html.duckduckgo.com/html/?q={query}\"\n    res = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    soup = BeautifulSoup(res.text, \"html.parser\")\n    links = [a['href'] for a in soup.find_all(\"a\", href=True) if 'http' in a['href']]\n    return {\"results\": links[:10]}\nelse:\n    return {\"error\": \"Unsupported source\"}"
        },
        "read_page": {
            "params": [
                "url"
            ],
            "body": "res = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nsoup = BeautifulSoup(res.text, \"html.parser\")\nreturn {\"html\": soup.prettify()}"
        },
        "scrape_links": {
            "params": [
                "url",
                "filter"
            ],
            "body": "res = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nsoup = BeautifulSoup(res.text, \"html.parser\")\npattern = re.compile(filter.replace(\"*\", \".*\") if filter else \".*\")\nlinks = [a['href'] for a in soup.find_all(\"a\", href=True) if pattern.match(a['href'])]\nreturn {\"links\": links}"
        },
        "extract_section": {
            "params": [
                "url",
                "selector"
            ],
            "body": "res = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nsoup = BeautifulSoup(res.text, \"html.parser\")\nsection = soup.select_one(selector)\nreturn {\"content\": section.get_text(strip=True) if section else \"\"}"
        },
        "summarize_article": {
            "params": [
                "url"
            ],
            "body": "res = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nsoup = BeautifulSoup(res.text, \"html.parser\")\nparagraphs = [p.get_text() for p in soup.find_all(\"p\")]\ntext = \" \".join(paragraphs)[:2000]\nreturn {\"summary\": text}  # Replace this with actual summary logic if needed"
        },
        "save_to_json": {
            "params": [
                "data",
                "filename"
            ],
            "body": "with open(filename, \"w\") as f:\n    json.dump(data, f, indent=2)\nreturn {\"status\": \"saved\", \"filename\": filename}"
        },
        "save_to_markdown": {
            "params": [
                "data",
                "filename"
            ],
            "body": "with open(filename, \"w\") as f:\n    if isinstance(data, str):\n        f.write(data)\n    else:\n        for key, value in data.items():\n            f.write(f\"## {key}\\n{value}\\n\\n\")\nreturn {\"status\": \"saved\", \"filename\": filename}"
        },
        "chain_to_tool": {
            "params": [
                "data",
                "tool",
                "action"
            ],
            "body": "# Stub: You\u2019ll integrate this with Orchestrate dispatcher or direct plugin call\nreturn {\"status\": \"ready_to_dispatch\", \"data\": data, \"target\": f\"{tool}/{action}\"}"
        }
    },
    "router": {}
}